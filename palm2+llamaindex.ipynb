{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "78b9fd250fa94c12badd1eaf16b3b0d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ae138fc1a6744b79df1518f7fd349c5",
              "IPY_MODEL_72e577b44dd242daaa9adfd717645ed4",
              "IPY_MODEL_65c973d85f934410bbfbe4bbd34259a7"
            ],
            "layout": "IPY_MODEL_71469dc4c7b24e39b9a9ba222090d165"
          }
        },
        "7ae138fc1a6744b79df1518f7fd349c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebd204629ae44ddca57d433a7ca20476",
            "placeholder": "​",
            "style": "IPY_MODEL_fecd58251f45415ab86708b2f2b80f08",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "72e577b44dd242daaa9adfd717645ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac5c1b509fd4407b8d86e79fb675156f",
            "max": 684,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_076a0eb9c9df40a09f7c4707f3e628a5",
            "value": 684
          }
        },
        "65c973d85f934410bbfbe4bbd34259a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e8984485dc54633947ab084cfce142f",
            "placeholder": "​",
            "style": "IPY_MODEL_933eae071a05441a9751db8812675c87",
            "value": " 684/684 [00:00&lt;00:00, 45.8kB/s]"
          }
        },
        "71469dc4c7b24e39b9a9ba222090d165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebd204629ae44ddca57d433a7ca20476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fecd58251f45415ab86708b2f2b80f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac5c1b509fd4407b8d86e79fb675156f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "076a0eb9c9df40a09f7c4707f3e628a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e8984485dc54633947ab084cfce142f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933eae071a05441a9751db8812675c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86ef5d756ed741c4be430305fb7f1078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85d7a8e5ed1e4ba5a6440bfa147c3611",
              "IPY_MODEL_aabcf9288b1a4cc09a4b916d2b721b66",
              "IPY_MODEL_203be9f1d261470d918709f8b6e88bd1"
            ],
            "layout": "IPY_MODEL_3ee9cba4e0204269a9b7775c93f59772"
          }
        },
        "85d7a8e5ed1e4ba5a6440bfa147c3611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0659fa2c7f754637987eb63f92fa475f",
            "placeholder": "​",
            "style": "IPY_MODEL_9feafcad7674449fa3e3120e1598b848",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "aabcf9288b1a4cc09a4b916d2b721b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18b19b21d30d43b796e86baed3a774e4",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8d5a910a76048938183dbc6ad654ab6",
            "value": 133466304
          }
        },
        "203be9f1d261470d918709f8b6e88bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68adfe7fd88d4efc9569d1148cd7898c",
            "placeholder": "​",
            "style": "IPY_MODEL_7daff023fa8d42ad87c9261aee441895",
            "value": " 133M/133M [00:01&lt;00:00, 180MB/s]"
          }
        },
        "3ee9cba4e0204269a9b7775c93f59772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0659fa2c7f754637987eb63f92fa475f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9feafcad7674449fa3e3120e1598b848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18b19b21d30d43b796e86baed3a774e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8d5a910a76048938183dbc6ad654ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68adfe7fd88d4efc9569d1148cd7898c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7daff023fa8d42ad87c9261aee441895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ae9bec960d74eeb80bad652000c5580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee19145339454cec87167b90c31badc5",
              "IPY_MODEL_62edc9af85f9424791b02ff19b1f3a48",
              "IPY_MODEL_d6f05e62d3754514b20215095eed4e49"
            ],
            "layout": "IPY_MODEL_f2e18eea3d944efe9163714ac6cf9aed"
          }
        },
        "ee19145339454cec87167b90c31badc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_122229f20e1e46f6a302e8177104a82a",
            "placeholder": "​",
            "style": "IPY_MODEL_319e410535144f498bfc4af5ff79c90f",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "62edc9af85f9424791b02ff19b1f3a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cfafb4c1ef244d7b05ffde5f4d7778c",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e38cd49ad404e92a7ef4a5c8280893e",
            "value": 366
          }
        },
        "d6f05e62d3754514b20215095eed4e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b1f04c6e924e8d8692b00aba5e76b4",
            "placeholder": "​",
            "style": "IPY_MODEL_c0a2588c629e42e5abb08e1c5f7d9489",
            "value": " 366/366 [00:00&lt;00:00, 23.1kB/s]"
          }
        },
        "f2e18eea3d944efe9163714ac6cf9aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122229f20e1e46f6a302e8177104a82a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "319e410535144f498bfc4af5ff79c90f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cfafb4c1ef244d7b05ffde5f4d7778c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e38cd49ad404e92a7ef4a5c8280893e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49b1f04c6e924e8d8692b00aba5e76b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0a2588c629e42e5abb08e1c5f7d9489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c2665f7a29b43439be156d12c1694d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b08b8368da0344b89e6091e77e43990d",
              "IPY_MODEL_8f953d7b5dda4eab85786e1ae4b4c3de",
              "IPY_MODEL_3a126fdb6ade425683dc27f763f359bb"
            ],
            "layout": "IPY_MODEL_380db9ac33d64921bd8ad2f7d2310c33"
          }
        },
        "b08b8368da0344b89e6091e77e43990d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f3dfd76e71f406aa6d2f447792ab055",
            "placeholder": "​",
            "style": "IPY_MODEL_fcc8ffa5b13b4c88b46311c45fe8ff75",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "8f953d7b5dda4eab85786e1ae4b4c3de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad49b3a055c24d2fb4ea0abdc20a7bb1",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e803d0b0560d4efc8d30e0643392dea5",
            "value": 231508
          }
        },
        "3a126fdb6ade425683dc27f763f359bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0aea0d6d4254195a44ccaaa59a3efb6",
            "placeholder": "​",
            "style": "IPY_MODEL_27548cc9b9224bf8b9306e7b78fd1ddf",
            "value": " 232k/232k [00:00&lt;00:00, 4.23MB/s]"
          }
        },
        "380db9ac33d64921bd8ad2f7d2310c33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f3dfd76e71f406aa6d2f447792ab055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc8ffa5b13b4c88b46311c45fe8ff75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad49b3a055c24d2fb4ea0abdc20a7bb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e803d0b0560d4efc8d30e0643392dea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0aea0d6d4254195a44ccaaa59a3efb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27548cc9b9224bf8b9306e7b78fd1ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ed5a562bb454e9cb30350ef8ccb6483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06d45b512dab421d9052af5e7b170d22",
              "IPY_MODEL_223e66fd26fc4883a980cb80b886a7e5",
              "IPY_MODEL_5415569985394a74b1f0a034142c1fa0"
            ],
            "layout": "IPY_MODEL_f459acedc6ee4cf1963065aafb7bd2d4"
          }
        },
        "06d45b512dab421d9052af5e7b170d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_676693eb60e442bca60f967d5df3bca7",
            "placeholder": "​",
            "style": "IPY_MODEL_36fc378524ea4178a4b754e29ba9efdb",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "223e66fd26fc4883a980cb80b886a7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd7e41f0078e480c9120aaeffd32d44c",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f8f3ffeb4aa45cf93014a6ae1a518c5",
            "value": 711396
          }
        },
        "5415569985394a74b1f0a034142c1fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d93ded780dbf451e9fc2fb3c5165e99e",
            "placeholder": "​",
            "style": "IPY_MODEL_4127b07d1223492598594fc971b1ae76",
            "value": " 711k/711k [00:00&lt;00:00, 28.0MB/s]"
          }
        },
        "f459acedc6ee4cf1963065aafb7bd2d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "676693eb60e442bca60f967d5df3bca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36fc378524ea4178a4b754e29ba9efdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd7e41f0078e480c9120aaeffd32d44c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f8f3ffeb4aa45cf93014a6ae1a518c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d93ded780dbf451e9fc2fb3c5165e99e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4127b07d1223492598594fc971b1ae76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8105c5e63ccb44ae906f5913f39569db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5cbb9e103e854f1887d9d0bca90ff2d3",
              "IPY_MODEL_1ba62b57453d4bca9c7e1907e23d1464",
              "IPY_MODEL_839e7082f2bf4458b5ac3b7cd3e88a74"
            ],
            "layout": "IPY_MODEL_28bbb306537d4209a456e1cb8e0743e8"
          }
        },
        "5cbb9e103e854f1887d9d0bca90ff2d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62542ee915894b90820128add69062ee",
            "placeholder": "​",
            "style": "IPY_MODEL_02964643d2c34a629cb56a0f580e5f7c",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "1ba62b57453d4bca9c7e1907e23d1464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6ee799b2a6a4b5d80e143070803dfac",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8eb61379d8914640a2ca93df71863f8a",
            "value": 125
          }
        },
        "839e7082f2bf4458b5ac3b7cd3e88a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1da75927c62b45ccb70c68f56f7f8c71",
            "placeholder": "​",
            "style": "IPY_MODEL_ca836b0801e045a386bfb9214fe77def",
            "value": " 125/125 [00:00&lt;00:00, 7.61kB/s]"
          }
        },
        "28bbb306537d4209a456e1cb8e0743e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62542ee915894b90820128add69062ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02964643d2c34a629cb56a0f580e5f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6ee799b2a6a4b5d80e143070803dfac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eb61379d8914640a2ca93df71863f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1da75927c62b45ccb70c68f56f7f8c71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca836b0801e045a386bfb9214fe77def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "odEMA4VoYX5M",
        "outputId": "88cacf92-8a71-4107-a2cc-4300c624b176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.6/792.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting pypdf\n",
            "  Downloading pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.4\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.2.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-ai-generativelanguage==0.3.3 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.3.3-py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.9/267.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.17.3)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.3.3->google-generativeai) (1.22.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.61.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai) (4.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.59.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai) (0.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2023.7.22)\n",
            "Installing collected packages: google-ai-generativelanguage, google-generativeai\n",
            "Successfully installed google-ai-generativelanguage-0.3.3 google-generativeai-0.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index\n",
        "!pip install pypdf\n",
        "!pip install google-generativeai\n",
        "!pip install transformers\n",
        "#!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.palm import PaLM\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import StorageContext, load_index_from_storage\n",
        "import os"
      ],
      "metadata": {
        "id": "H9nZGcvyYkg3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "6ODCy3S0Yz3F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"data\").load_data()"
      ],
      "metadata": {
        "id": "LqwGX9s5Y457"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bgzhWjZaPd6",
        "outputId": "84fa4f30-720a-48d6-ee7b-cdb97dd03ae5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id_='d56406dd-c823-4e4b-9a18-1705bbf9043f', embedding=None, metadata={'page_label': '1', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='70fc309ce7159bd0261abb9f4f8b31b2529589ecfa87a46af8227738adc0ffe9', text='YOLOv4: Optimal Speed and Accuracy of Object Detection\\nAlexey Bochkovskiy∗\\nalexeyab84@gmail.comChien-Yao Wang∗\\nInstitute of Information Science\\nAcademia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.twHong-Yuan Mark Liao\\nInstitute of Information Science\\nAcademia Sinica, Taiwan\\nliao@iis.sinica.edu.tw\\nAbstract\\nThere are a huge number of features which are said to\\nimprove Convolutional Neural Network (CNN) accuracy.\\nPractical testing of combinations of such features on large\\ndatasets, and theoretical justiﬁcation of the result, is re-\\nquired. Some features operate on certain models exclusively\\nand for certain problems exclusively, or only for small-scale\\ndatasets; while some features, such as batch-normalization\\nand residual-connections, are applicable to the majority of\\nmodels, tasks, and datasets. We assume that such universal\\nfeatures include Weighted-Residual-Connections (WRC),\\nCross-Stage-Partial-connections (CSP), Cross mini-Batch\\nNormalization (CmBN), Self-adversarial-training (SAT)\\nand Mish-activation. We use new features: WRC, CSP ,\\nCmBN, SAT, Mish activation, Mosaic data augmentation,\\nCmBN, DropBlock regularization, and CIoU loss, and com-\\nbine some of them to achieve state-of-the-art results: 43.5%\\nAP (65.7% AP 50) for the MS COCO dataset at a real-\\ntime speed of∼65 FPS on Tesla V100. Source code is at\\nhttps://github.com/AlexeyAB/darknet .\\n1. Introduction\\nThe majority of CNN-based object detectors are largely\\napplicable only for recommendation systems. For example,\\nsearching for free parking spaces via urban video cameras\\nis executed by slow accurate models, whereas car collision\\nwarning is related to fast inaccurate models. Improving\\nthe real-time object detector accuracy enables using them\\nnot only for hint generating recommendation systems, but\\nalso for stand-alone process management and human input\\nreduction. Real-time object detector operation on conven-\\ntional Graphics Processing Units (GPU) allows their mass\\nusage at an affordable price. The most accurate modern\\nneural networks do not operate in real time and require large\\nnumber of GPUs for training with a large mini-batch-size.\\nWe address such problems through creating a CNN that op-\\nerates in real-time on a conventional GPU, and for which\\ntraining requires only one conventional GPU.\\nFigure 1: Comparison of the proposed YOLOv4 and other\\nstate-of-the-art object detectors. YOLOv4 runs twice faster\\nthan EfﬁcientDet with comparable performance. Improves\\nYOLOv3’s AP and FPS by 10% and 12%, respectively.\\nThe main goal of this work is designing a fast operating\\nspeed of an object detector in production systems and opti-\\nmization for parallel computations, rather than the low com-\\nputation volume theoretical indicator (BFLOP). We hope\\nthat the designed object can be easily trained and used. For\\nexample, anyone who uses a conventional GPU to train and\\ntest can achieve real-time, high quality, and convincing ob-\\nject detection results, as the YOLOv4 results shown in Fig-\\nure 1. Our contributions are summarized as follows:\\n1. We develope an efﬁcient and powerful object detection\\nmodel. It makes everyone can use a 1080 Ti or 2080 Ti\\nGPU to train a super fast and accurate object detector.\\n2. We verify the inﬂuence of state-of-the-art Bag-of-\\nFreebies and Bag-of-Specials methods of object detec-\\ntion during the detector training.\\n3. We modify state-of-the-art methods and make them\\nmore effecient and suitable for single GPU training,\\nincluding CBN [89], PAN [49], SAM [85], etc.\\n1arXiv:2004.10934v1  [cs.CV]  23 Apr 2020', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='44fb3132-fd7e-4b43-97f4-1b7ab38798d7', embedding=None, metadata={'page_label': '2', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fe19cb7546c189e315d1f26310436d2dbd51d29173c8142096485966696a93e2', text='Figure 2: Object detector.\\n2. Related work\\n2.1. Object detection models\\nA modern detector is usually composed of two parts,\\na backbone which is pre-trained on ImageNet and a head\\nwhich is used to predict classes and bounding boxes of ob-\\njects. For those detectors running on GPU platform, their\\nbackbone could be VGG [68], ResNet [26], ResNeXt [86],\\nor DenseNet [30]. For those detectors running on CPU plat-\\nform, their backbone could be SqueezeNet [31], MobileNet\\n[28, 66, 27, 74], or ShufﬂeNet [97, 53]. As to the head part,\\nit is usually categorized into two kinds, i.e., one-stage object\\ndetector and two-stage object detector. The most represen-\\ntative two-stage object detector is the R-CNN [19] series,\\nincluding fast R-CNN [18], faster R-CNN [64], R-FCN [9],\\nand Libra R-CNN [58]. It is also possible to make a two-\\nstage object detector an anchor-free object detector, such as\\nRepPoints [87]. As for one-stage object detector, the most\\nrepresentative models are YOLO [61, 62, 63], SSD [50],\\nand RetinaNet [45]. In recent years, anchor-free one-stage\\nobject detectors are developed. The detectors of this sort are\\nCenterNet [13], CornerNet [37, 38], FCOS [78], etc. Object\\ndetectors developed in recent years often insert some lay-\\ners between backbone and head, and these layers are usu-\\nally used to collect feature maps from different stages. We\\ncan call it the neck of an object detector. Usually, a neck\\nis composed of several bottom-up paths and several top-\\ndown paths. Networks equipped with this mechanism in-\\nclude Feature Pyramid Network (FPN) [44], Path Aggrega-\\ntion Network (PAN) [49], BiFPN [77], and NAS-FPN [17].In addition to the above models, some researchers put their\\nemphasis on directly building a new backbone (DetNet [43],\\nDetNAS [7]) or a new whole model (SpineNet [12], HitDe-\\ntector [20]) for object detection.\\nTo sum up, an ordinary object detector is composed of\\nseveral parts:\\n•Input : Image, Patches, Image Pyramid\\n•Backbones : VGG16 [68], ResNet-50 [26], SpineNet\\n[12], EfﬁcientNet-B0/B7 [75], CSPResNeXt50 [81],\\nCSPDarknet53 [81]\\n•Neck :\\n•Additional blocks : SPP [25], ASPP [5], RFB\\n[47], SAM [85]\\n•Path-aggregation blocks : FPN [44], PAN [49],\\nNAS-FPN [17], Fully-connected FPN, BiFPN\\n[77], ASFF [48], SFAM [98]\\n•Heads: :\\n•Dense Prediction (one-stage) :\\n◦RPN [64], SSD [50], YOLO [61], RetinaNet\\n[45] (anchor based)\\n◦CornerNet [37], CenterNet [13], MatrixNet\\n[60], FCOS [78] (anchor free)\\n•Sparse Prediction (two-stage) :\\n◦Faster R-CNN [64], R-FCN [9], Mask R-\\nCNN [23] (anchor based)\\n◦RepPoints [87] (anchor free)\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1d089fd9-2bde-4014-a44b-6340e55900ec', embedding=None, metadata={'page_label': '3', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='47cd868a974cccd1c1e513206513d2a0155cbc5bed6e488211cc3b7b8d3c132c', text='2.2. Bag of freebies\\nUsually, a conventional object detector is trained off-\\nline. Therefore, researchers always like to take this advan-\\ntage and develop better training methods which can make\\nthe object detector receive better accuracy without increas-\\ning the inference cost. We call these methods that only\\nchange the training strategy or only increase the training\\ncost as “bag of freebies.” What is often adopted by object\\ndetection methods and meets the deﬁnition of bag of free-\\nbies is data augmentation. The purpose of data augmenta-\\ntion is to increase the variability of the input images, so that\\nthe designed object detection model has higher robustness\\nto the images obtained from different environments. For\\nexamples, photometric distortions and geometric distortions\\nare two commonly used data augmentation method and they\\ndeﬁnitely beneﬁt the object detection task. In dealing with\\nphotometric distortion, we adjust the brightness, contrast,\\nhue, saturation, and noise of an image. For geometric dis-\\ntortion, we add random scaling, cropping, ﬂipping, and ro-\\ntating.\\nThe data augmentation methods mentioned above are all\\npixel-wise adjustments, and all original pixel information in\\nthe adjusted area is retained. In addition, some researchers\\nengaged in data augmentation put their emphasis on sim-\\nulating object occlusion issues. They have achieved good\\nresults in image classiﬁcation and object detection. For ex-\\nample, random erase [100] and CutOut [11] can randomly\\nselect the rectangle region in an image and ﬁll in a random\\nor complementary value of zero. As for hide-and-seek [69]\\nand grid mask [6], they randomly or evenly select multiple\\nrectangle regions in an image and replace them to all ze-\\nros. If similar concepts are applied to feature maps, there\\nare DropOut [71], DropConnect [80], and DropBlock [16]\\nmethods. In addition, some researchers have proposed the\\nmethods of using multiple images together to perform data\\naugmentation. For example, MixUp [92] uses two images\\nto multiply and superimpose with different coefﬁcient ra-\\ntios, and then adjusts the label with these superimposed ra-\\ntios. As for CutMix [91], it is to cover the cropped image\\nto rectangle region of other images, and adjusts the label\\naccording to the size of the mix area. In addition to the\\nabove mentioned methods, style transfer GAN [15] is also\\nused for data augmentation, and such usage can effectively\\nreduce the texture bias learned by CNN.\\nDifferent from the various approaches proposed above,\\nsome other bag of freebies methods are dedicated to solving\\nthe problem that the semantic distribution in the dataset may\\nhave bias. In dealing with the problem of semantic distri-\\nbution bias, a very important issue is that there is a problem\\nof data imbalance between different classes, and this prob-\\nlem is often solved by hard negative example mining [72]\\nor online hard example mining [67] in two-stage object de-\\ntector. But the example mining method is not applicableto one-stage object detector, because this kind of detector\\nbelongs to the dense prediction architecture. Therefore Lin\\net al . [45] proposed focal loss to deal with the problem\\nof data imbalance existing between various classes. An-\\nother very important issue is that it is difﬁcult to express the\\nrelationship of the degree of association between different\\ncategories with the one-hot hard representation. This rep-\\nresentation scheme is often used when executing labeling.\\nThe label smoothing proposed in [73] is to convert hard la-\\nbel into soft label for training, which can make model more\\nrobust. In order to obtain a better soft label, Islam et al. [33]\\nintroduced the concept of knowledge distillation to design\\nthe label reﬁnement network.\\nThe last bag of freebies is the objective function of\\nBounding Box (BBox) regression. The traditional object\\ndetector usually uses Mean Square Error (MSE) to di-\\nrectly perform regression on the center point coordinates\\nand height and width of the BBox, i.e., {xcenter ,ycenter ,\\nw,h}, or the upper left point and the lower right point,\\ni.e.,{xtopleft,ytopleft,xbottom right ,ybottom right}. As\\nfor anchor-based method, it is to estimate the correspond-\\ning offset, for example {xcenter offset ,ycenter offset ,\\nwoffset ,hoffset}and{xtopleftoffset ,ytopleftoffset ,\\nxbottom right offset ,ybottom right offset}. However, to di-\\nrectly estimate the coordinate values of each point of the\\nBBox is to treat these points as independent variables, but\\nin fact does not consider the integrity of the object itself. In\\norder to make this issue processed better, some researchers\\nrecently proposed IoU loss [90], which puts the coverage of\\npredicted BBox area and ground truth BBox area into con-\\nsideration. The IoU loss computing process will trigger the\\ncalculation of the four coordinate points of the BBox by ex-\\necuting IoU with the ground truth, and then connecting the\\ngenerated results into a whole code. Because IoU is a scale\\ninvariant representation, it can solve the problem that when\\ntraditional methods calculate the l1orl2loss of{x,y,w,\\nh}, the loss will increase with the scale. Recently, some\\nresearchers have continued to improve IoU loss. For exam-\\nple, GIoU loss [65] is to include the shape and orientation\\nof object in addition to the coverage area. They proposed to\\nﬁnd the smallest area BBox that can simultaneously cover\\nthe predicted BBox and ground truth BBox, and use this\\nBBox as the denominator to replace the denominator origi-\\nnally used in IoU loss. As for DIoU loss [99], it additionally\\nconsiders the distance of the center of an object, and CIoU\\nloss [99], on the other hand simultaneously considers the\\noverlapping area, the distance between center points, and\\nthe aspect ratio. CIoU can achieve better convergence speed\\nand accuracy on the BBox regression problem.\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ae3e9c53-f902-4aa8-a672-9b6001f21316', embedding=None, metadata={'page_label': '4', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='c119c69ecd453aecbbddfed7049e95b2a96e1e4cdcde227ed64e73767a75c3e8', text='2.3. Bag of specials\\nFor those plugin modules and post-processing methods\\nthat only increase the inference cost by a small amount\\nbut can signiﬁcantly improve the accuracy of object detec-\\ntion, we call them “bag of specials”. Generally speaking,\\nthese plugin modules are for enhancing certain attributes in\\na model, such as enlarging receptive ﬁeld, introducing at-\\ntention mechanism, or strengthening feature integration ca-\\npability, etc., and post-processing is a method for screening\\nmodel prediction results.\\nCommon modules that can be used to enhance recep-\\ntive ﬁeld are SPP [25], ASPP [5], and RFB [47]. The\\nSPP module was originated from Spatial Pyramid Match-\\ning (SPM) [39], and SPMs original method was to split fea-\\nture map into several d×dequal blocks, where dcan be\\n{1,2,3,...}, thus forming spatial pyramid, and then extract-\\ning bag-of-word features. SPP integrates SPM into CNN\\nand use max-pooling operation instead of bag-of-word op-\\neration. Since the SPP module proposed by He et al. [25]\\nwill output one dimensional feature vector, it is infeasible to\\nbe applied in Fully Convolutional Network (FCN). Thus in\\nthe design of YOLOv3 [63], Redmon and Farhadi improve\\nSPP module to the concatenation of max-pooling outputs\\nwith kernel size k×k, wherek={1,5,9,13}, and stride\\nequals to 1. Under this design, a relatively large k×kmax-\\npooling effectively increase the receptive ﬁeld of backbone\\nfeature. After adding the improved version of SPP module,\\nYOLOv3-608 upgrades AP 50by 2.7% on the MS COCO\\nobject detection task at the cost of 0.5% extra computation.\\nThe difference in operation between ASPP [5] module and\\nimproved SPP module is mainly from the original k×kker-\\nnel size, max-pooling of stride equals to 1 to several 3×3\\nkernel size, dilated ratio equals to k, and stride equals to 1\\nin dilated convolution operation. RFB module is to use sev-\\neral dilated convolutions of k×kkernel, dilated ratio equals\\ntok, and stride equals to 1 to obtain a more comprehensive\\nspatial coverage than ASPP. RFB [47] only costs 7% extra\\ninference time to increase the AP 50of SSD on MS COCO\\nby 5.7%.\\nThe attention module that is often used in object detec-\\ntion is mainly divided into channel-wise attention and point-\\nwise attention, and the representatives of these two atten-\\ntion models are Squeeze-and-Excitation (SE) [29] and Spa-\\ntial Attention Module (SAM) [85], respectively. Although\\nSE module can improve the power of ResNet50 in the Im-\\nageNet image classiﬁcation task 1% top-1 accuracy at the\\ncost of only increasing the computational effort by 2%, but\\non a GPU usually it will increase the inference time by\\nabout 10%, so it is more appropriate to be used in mobile\\ndevices. But for SAM, it only needs to pay 0.1% extra cal-\\nculation and it can improve ResNet50-SE 0.5% top-1 accu-\\nracy on the ImageNet image classiﬁcation task. Best of all,\\nit does not affect the speed of inference on the GPU at all.In terms of feature integration, the early practice is to use\\nskip connection [51] or hyper-column [22] to integrate low-\\nlevel physical feature to high-level semantic feature. Since\\nmulti-scale prediction methods such as FPN have become\\npopular, many lightweight modules that integrate different\\nfeature pyramid have been proposed. The modules of this\\nsort include SFAM [98], ASFF [48], and BiFPN [77]. The\\nmain idea of SFAM is to use SE module to execute channel-\\nwise level re-weighting on multi-scale concatenated feature\\nmaps. As for ASFF, it uses softmax as point-wise level re-\\nweighting and then adds feature maps of different scales.\\nIn BiFPN, the multi-input weighted residual connections is\\nproposed to execute scale-wise level re-weighting, and then\\nadd feature maps of different scales.\\nIn the research of deep learning, some people put their\\nfocus on searching for good activation function. A good\\nactivation function can make the gradient more efﬁciently\\npropagated, and at the same time it will not cause too\\nmuch extra computational cost. In 2010, Nair and Hin-\\nton [56] propose ReLU to substantially solve the gradient\\nvanish problem which is frequently encountered in tradi-\\ntional tanh and sigmoid activation function. Subsequently,\\nLReLU [54], PReLU [24], ReLU6 [28], Scaled Exponential\\nLinear Unit (SELU) [35], Swish [59], hard-Swish [27], and\\nMish [55], etc., which are also used to solve the gradient\\nvanish problem, have been proposed. The main purpose of\\nLReLU and PReLU is to solve the problem that the gradi-\\nent of ReLU is zero when the output is less than zero. As\\nfor ReLU6 and hard-Swish, they are specially designed for\\nquantization networks. For self-normalizing a neural net-\\nwork, the SELU activation function is proposed to satisfy\\nthe goal. One thing to be noted is that both Swish and Mish\\nare continuously differentiable activation function.\\nThe post-processing method commonly used in deep-\\nlearning-based object detection is NMS, which can be used\\nto ﬁlter those BBoxes that badly predict the same ob-\\nject, and only retain the candidate BBoxes with higher re-\\nsponse. The way NMS tries to improve is consistent with\\nthe method of optimizing an objective function. The orig-\\ninal method proposed by NMS does not consider the con-\\ntext information, so Girshick et al. [19] added classiﬁcation\\nconﬁdence score in R-CNN as a reference, and according to\\nthe order of conﬁdence score, greedy NMS was performed\\nin the order of high score to low score. As for soft NMS [1],\\nit considers the problem that the occlusion of an object may\\ncause the degradation of conﬁdence score in greedy NMS\\nwith IoU score. The DIoU NMS [99] developers way of\\nthinking is to add the information of the center point dis-\\ntance to the BBox screening process on the basis of soft\\nNMS. It is worth mentioning that, since none of above post-\\nprocessing methods directly refer to the captured image fea-\\ntures, post-processing is no longer required in the subse-\\nquent development of an anchor-free method.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='75fd3563-d644-44ab-b47b-66de3d571d72', embedding=None, metadata={'page_label': '5', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='339b1e7489c55a39169db666747bc07b89b66b648bd3bc82aa0ae896c6e24004', text='Table 1: Parameters of neural networks for image classiﬁcation.\\nBackbone modelInput network\\nresolutionReceptive\\nﬁeld sizeParametersAverage size\\nof layer output\\n(WxHxC)BFLOPs\\n(512x512 network resolution)FPS\\n(GPU RTX 2070)\\nCSPResNext50 512x512 425x425 20.6 M 1058 K 31 (15.5 FMA) 62\\nCSPDarknet53 512x512 725x725 27.6 M 950 K 52 (26.0 FMA) 66\\nEfﬁcientNet-B3 (ours) 512x512 1311x1311 12.0 M 668 K 11 (5.5 FMA) 26\\n3. Methodology\\nThe basic aim is fast operating speed of neural network,\\nin production systems and optimization for parallel compu-\\ntations, rather than the low computation volume theoreti-\\ncal indicator (BFLOP). We present two options of real-time\\nneural networks:\\n•For GPU we use a small number of groups (1 - 8) in\\nconvolutional layers: CSPResNeXt50 / CSPDarknet53\\n•For VPU - we use grouped-convolution, but we re-\\nfrain from using Squeeze-and-excitement (SE) blocks\\n- speciﬁcally this includes the following models:\\nEfﬁcientNet-lite / MixNet [76] / GhostNet [21] / Mo-\\nbileNetV3\\n3.1. Selection of architecture\\nOur objective is to ﬁnd the optimal balance among the in-\\nput network resolution, the convolutional layer number, the\\nparameter number (ﬁlter size2* ﬁlters * channel / groups),\\nand the number of layer outputs (ﬁlters). For instance, our\\nnumerous studies demonstrate that the CSPResNext50 is\\nconsiderably better compared to CSPDarknet53 in terms\\nof object classiﬁcation on the ILSVRC2012 (ImageNet)\\ndataset [10]. However, conversely, the CSPDarknet53 is\\nbetter compared to CSPResNext50 in terms of detecting ob-\\njects on the MS COCO dataset [46].\\nThe next objective is to select additional blocks for in-\\ncreasing the receptive ﬁeld and the best method of parame-\\nter aggregation from different backbone levels for different\\ndetector levels: e.g. FPN, PAN, ASFF, BiFPN.\\nA reference model which is optimal for classiﬁcation is\\nnot always optimal for a detector. In contrast to the classi-\\nﬁer, the detector requires the following:\\n•Higher input network size (resolution) – for detecting\\nmultiple small-sized objects\\n•More layers – for a higher receptive ﬁeld to cover the\\nincreased size of input network\\n•More parameters – for greater capacity of a model to\\ndetect multiple objects of different sizes in a single im-\\nageHypothetically speaking, we can assume that a model\\nwith a larger receptive ﬁeld size (with a larger number of\\nconvolutional layers 3×3) and a larger number of parame-\\nters should be selected as the backbone. Table 1 shows the\\ninformation of CSPResNeXt50, CSPDarknet53, and Efﬁ-\\ncientNet B3. The CSPResNext50 contains only 16 convo-\\nlutional layers 3×3, a425×425receptive ﬁeld and 20.6\\nM parameters, while CSPDarknet53 contains 29 convolu-\\ntional layers 3×3, a725×725receptive ﬁeld and 27.6\\nM parameters. This theoretical justiﬁcation, together with\\nour numerous experiments, show that CSPDarknet53 neu-\\nral network is the optimal model of the two as the backbone\\nfor a detector.\\nThe inﬂuence of the receptive ﬁeld with different sizes is\\nsummarized as follows:\\n•Up to the object size - allows viewing the entire object\\n•Up to network size - allows viewing the context around\\nthe object\\n•Exceeding the network size - increases the number of\\nconnections between the image point and the ﬁnal ac-\\ntivation\\nWe add the SPP block over the CSPDarknet53, since it\\nsigniﬁcantly increases the receptive ﬁeld, separates out the\\nmost signiﬁcant context features and causes almost no re-\\nduction of the network operation speed. We use PANet as\\nthe method of parameter aggregation from different back-\\nbone levels for different detector levels, instead of the FPN\\nused in YOLOv3.\\nFinally, we choose CSPDarknet53 backbone, SPP addi-\\ntional module, PANet path-aggregation neck, and YOLOv3\\n(anchor based) head as the architecture of YOLOv4.\\nIn the future we plan to expand signiﬁcantly the content\\nof Bag of Freebies (BoF) for the detector, which theoreti-\\ncally can address some problems and increase the detector\\naccuracy, and sequentially check the inﬂuence of each fea-\\nture in an experimental fashion.\\nWe do not use Cross-GPU Batch Normalization (CGBN\\nor SyncBN) or expensive specialized devices. This al-\\nlows anyone to reproduce our state-of-the-art outcomes on\\na conventional graphic processor e.g. GTX 1080Ti or RTX\\n2080Ti.\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='57bf86f6-0a24-4743-a652-c9c9b8e3080c', embedding=None, metadata={'page_label': '6', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8ac4c335e1266c0aada50993d13e5b827ee9803c21e088fb8204327021e0f4d4', text='3.2. Selection of BoF and BoS\\nFor improving the object detection training, a CNN usu-\\nally uses the following:\\n•Activations : ReLU, leaky-ReLU, parametric-ReLU,\\nReLU6, SELU, Swish, or Mish\\n•Bounding box regression loss : MSE, IoU, GIoU,\\nCIoU, DIoU\\n•Data augmentation : CutOut, MixUp, CutMix\\n•Regularization method : DropOut, DropPath [36],\\nSpatial DropOut [79], or DropBlock\\n•Normalization of the network activations by their\\nmean and variance : Batch Normalization (BN) [32],\\nCross-GPU Batch Normalization (CGBN or SyncBN)\\n[93], Filter Response Normalization (FRN) [70], or\\nCross-Iteration Batch Normalization (CBN) [89]\\n•Skip-connections : Residual connections, Weighted\\nresidual connections, Multi-input weighted residual\\nconnections, or Cross stage partial connections (CSP)\\nAs for training activation function, since PReLU and\\nSELU are more difﬁcult to train, and ReLU6 is speciﬁcally\\ndesigned for quantization network, we therefore remove the\\nabove activation functions from the candidate list. In the\\nmethod of reqularization, the people who published Drop-\\nBlock have compared their method with other methods in\\ndetail, and their regularization method has won a lot. There-\\nfore, we did not hesitate to choose DropBlock as our reg-\\nularization method. As for the selection of normalization\\nmethod, since we focus on a training strategy that uses only\\none GPU, syncBN is not considered.\\n3.3. Additional improvements\\nIn order to make the designed detector more suitable for\\ntraining on single GPU, we made additional design and im-\\nprovement as follows:\\n•We introduce a new method of data augmentation Mo-\\nsaic, and Self-Adversarial Training (SAT)\\n•We select optimal hyper-parameters while applying\\ngenetic algorithms\\n•We modify some exsiting methods to make our design\\nsuitble for efﬁcient training and detection - modiﬁed\\nSAM, modiﬁed PAN, and Cross mini-Batch Normal-\\nization (CmBN)\\nMosaic represents a new data augmentation method that\\nmixes 4 training images. Thus 4 different contexts are\\nFigure 3: Mosaic represents a new method of data augmen-\\ntation.\\nmixed, while CutMix mixes only 2 input images. This al-\\nlows detection of objects outside their normal context. In\\naddition, batch normalization calculates activation statistics\\nfrom 4 different images on each layer. This signiﬁcantly\\nreduces the need for a large mini-batch size.\\nSelf-Adversarial Training (SAT) also represents a new\\ndata augmentation technique that operates in 2 forward\\nbackward stages. In the 1st stage the neural network alters\\nthe original image instead of the network weights. In this\\nway the neural network executes an adversarial attack on it-\\nself, altering the original image to create the deception that\\nthere is no desired object on the image. In the 2nd stage, the\\nneural network is trained to detect an object on this modiﬁed\\nimage in the normal way.\\nFigure 4: Cross mini-Batch Normalization.\\nCmBN represents a CBN modiﬁed version, as shown\\nin Figure 4, deﬁned as Cross mini-Batch Normalization\\n(CmBN). This collects statistics only between mini-batches\\nwithin a single batch.\\nWe modify SAM from spatial-wise attention to point-\\nwise attention, and replace shortcut connection of PAN to\\nconcatenation, as shown in Figure 5 and Figure 6, respec-\\ntively.\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7598c71b-cdc1-4b92-85a3-21f5467d6f72', embedding=None, metadata={'page_label': '7', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='1f4125392302a754fb2e0f4ac50ec744b0bb758e864599f025c275b25a8db6aa', text='Figure 5: Modiﬁed SAM.\\nFigure 6: Modiﬁed PAN.\\n3.4. YOLOv4\\nIn this section, we shall elaborate the details of YOLOv4.\\nYOLOv4 consists of :\\n•Backbone: CSPDarknet53 [81]\\n•Neck: SPP [25], PAN [49]\\n•Head: YOLOv3 [63]\\nYOLO v4 uses :\\n•Bag of Freebies (BoF) for backbone: CutMix and\\nMosaic data augmentation, DropBlock regularization,\\nClass label smoothing\\n•Bag of Specials (BoS) for backbone: Mish activa-\\ntion, Cross-stage partial connections (CSP), Multi-\\ninput weighted residual connections (MiWRC)\\n•Bag of Freebies (BoF) for detector: CIoU-loss,\\nCmBN, DropBlock regularization, Mosaic data aug-\\nmentation, Self-Adversarial Training, Eliminate grid\\nsensitivity, Using multiple anchors for a single ground\\ntruth, Cosine annealing scheduler [52], Optimal hyper-\\nparameters, Random training shapes\\n•Bag of Specials (BoS) for detector: Mish activation,\\nSPP-block, SAM-block, PAN path-aggregation block,\\nDIoU-NMS4. Experiments\\nWe test the inﬂuence of different training improve-\\nment techniques on accuracy of the classiﬁer on ImageNet\\n(ILSVRC 2012 val) dataset, and then on the accuracy of the\\ndetector on MS COCO (test-dev 2017) dataset.\\n4.1. Experimental setup\\nIn ImageNet image classiﬁcation experiments, the de-\\nfault hyper-parameters are as follows: the training steps is\\n8,000,000; the batch size and the mini-batch size are 128\\nand 32, respectively; the polynomial decay learning rate\\nscheduling strategy is adopted with initial learning rate 0.1;\\nthe warm-up steps is 1000; the momentum and weight de-\\ncay are respectively set as 0.9 and 0.005. All of our BoS\\nexperiments use the same hyper-parameter as the default\\nsetting, and in the BoF experiments, we add an additional\\n50% training steps. In the BoF experiments, we verify\\nMixUp, CutMix, Mosaic, Bluring data augmentation, and\\nlabel smoothing regularization methods. In the BoS experi-\\nments, we compared the effects of LReLU, Swish, and Mish\\nactivation function. All experiments are trained with a 1080\\nTi or 2080 Ti GPU.\\nIn MS COCO object detection experiments, the de-\\nfault hyper-parameters are as follows: the training steps is\\n500,500; the step decay learning rate scheduling strategy is\\nadopted with initial learning rate 0.01 and multiply with a\\nfactor 0.1 at the 400,000 steps and the 450,000 steps, re-\\nspectively; The momentum and weight decay are respec-\\ntively set as 0.9 and 0.0005. All architectures use a sin-\\ngle GPU to execute multi-scale training in the batch size\\nof 64 while mini-batch size is 8 or 4 depend on the ar-\\nchitectures and GPU memory limitation. Except for us-\\ning genetic algorithm for hyper-parameter search experi-\\nments, all other experiments use default setting. Genetic\\nalgorithm used YOLOv3-SPP to train with GIoU loss and\\nsearch 300 epochs for min-val 5k sets. We adopt searched\\nlearning rate 0.00261, momentum 0.949, IoU threshold for\\nassigning ground truth 0.213, and loss normalizer 0.07 for\\ngenetic algorithm experiments. We have veriﬁed a large\\nnumber of BoF, including grid sensitivity elimination, mo-\\nsaic data augmentation, IoU threshold, genetic algorithm,\\nclass label smoothing, cross mini-batch normalization, self-\\nadversarial training, cosine annealing scheduler, dynamic\\nmini-batch size, DropBlock, Optimized Anchors, different\\nkind of IoU losses. We also conduct experiments on various\\nBoS, including Mish, SPP, SAM, RFB, BiFPN, and Gaus-\\nsian YOLO [8]. For all experiments, we only use one GPU\\nfor training, so techniques such as syncBN that optimizes\\nmultiple GPUs are not used.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='76d13415-f487-4885-9716-0966da05ebb9', embedding=None, metadata={'page_label': '8', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d8899327ddec3ec66cd001cd4593ec4e539841d6d115b78cc6fb03629bc3157e', text='4.2. Inﬂuence of different features on Classiﬁer\\ntraining\\nFirst, we study the inﬂuence of different features on\\nclassiﬁer training; speciﬁcally, the inﬂuence of Class la-\\nbel smoothing, the inﬂuence of different data augmentation\\ntechniques, bilateral blurring, MixUp, CutMix and Mosaic,\\nas shown in Fugure 7, and the inﬂuence of different activa-\\ntions, such as Leaky-ReLU (by default), Swish, and Mish.\\nFigure 7: Various method of data augmentation.\\nIn our experiments, as illustrated in Table 2, the clas-\\nsiﬁer’s accuracy is improved by introducing the features\\nsuch as: CutMix and Mosaic data augmentation, Class la-\\nbel smoothing, and Mish activation. As a result, our BoF-\\nbackbone (Bag of Freebies) for classiﬁer training includes\\nthe following: CutMix and Mosaic data augmentation and\\nClass label smoothing. In addition we use Mish activation\\nas a complementary option, as shown in Table 2 and Table\\n3.\\nTable 2: Inﬂuence of BoF and Mish on the CSPResNeXt-50 clas-\\nsiﬁer accuracy.\\nMixUp CutMix Mosaic BluringLabel\\nSmoothingSwish Mish Top-1 Top-5\\n77.9% 94.0%\\n✓ 77.2% 94.0%\\n✓ 78.0% 94.3%\\n✓ 78.1% 94.5%\\n✓ 77.5% 93.8%\\n✓ 78.1% 94.4%\\n✓ 64.5% 86.0%\\n✓78.9% 94.5%\\n✓ ✓ ✓ 78.5% 94.8%\\n✓ ✓ ✓ ✓ 79.8% 95.2%\\nTable 3: Inﬂuence of BoF and Mish on the CSPDarknet-53 classi-\\nﬁer accuracy.\\nMixUp CutMix Mosaic BluringLabel\\nSmoothingSwish Mish Top-1 Top-5\\n77.2% 93.6%\\n✓ ✓ ✓ 77.8% 94.4%\\n✓ ✓ ✓ ✓ 78.7% 94.8%4.3. Inﬂuence of different features on Detector\\ntraining\\nFurther study concerns the inﬂuence of different Bag-of-\\nFreebies (BoF-detector) on the detector training accuracy,\\nas shown in Table 4. We signiﬁcantly expand the BoF list\\nthrough studying different features that increase the detector\\naccuracy without affecting FPS:\\n•S: Eliminate grid sensitivity the equation bx=σ(tx)+\\ncx,by=σ(ty)+cy, wherecxandcyare always whole\\nnumbers, is used in YOLOv3 for evaluating the ob-\\nject coordinates, therefore, extremely high txabsolute\\nvalues are required for the bxvalue approaching the\\ncxorcx+ 1 values. We solve this problem through\\nmultiplying the sigmoid by a factor exceeding 1.0, so\\neliminating the effect of grid on which the object is\\nundetectable.\\n•M: Mosaic data augmentation - using the 4-image mo-\\nsaic during training instead of single image\\n•IT: IoU threshold - using multiple anchors for a single\\nground truth IoU (truth, anchor) >IoU threshold\\n•GA: Genetic algorithms - using genetic algorithms for\\nselecting the optimal hyperparameters during network\\ntraining on the ﬁrst 10% of time periods\\n•LS: Class label smoothing - using class label smooth-\\ning for sigmoid activation\\n•CBN: CmBN - using Cross mini-Batch Normalization\\nfor collecting statistics inside the entire batch, instead\\nof collecting statistics inside a single mini-batch\\n•CA: Cosine annealing scheduler - altering the learning\\nrate during sinusoid training\\n•DM: Dynamic mini-batch size - automatic increase of\\nmini-batch size during small resolution training by us-\\ning Random training shapes\\n•OA: Optimized Anchors - using the optimized anchors\\nfor training with the 512x512 network resolution\\n•GIoU, CIoU, DIoU, MSE - using different loss algo-\\nrithms for bounded box regression\\nFurther study concerns the inﬂuence of different Bag-\\nof-Specials (BoS-detector) on the detector training accu-\\nracy, including PAN, RFB, SAM, Gaussian YOLO (G), and\\nASFF, as shown in Table 5. In our experiments, the detector\\ngets best performance when using SPP, PAN, and SAM.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='2667ccb6-2272-4113-a985-dd01fd1df872', embedding=None, metadata={'page_label': '9', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2a10e8f34c8b40f7f3493db61dcd8166d365e51d2a05531fa9cdd6936da6d47b', text='Table 4: Ablation Studies of Bag-of-Freebies. (CSPResNeXt50-PANet-SPP, 512x512).\\nS M IT GA LS CBN CA DM OA loss AP AP 50 AP75\\nMSE 38.0% 60.0% 40.8%\\n✓ MSE 37.7% 59.9% 40.5%\\n✓ MSE 39.1% 61.8% 42.0%\\n✓ MSE 36.9% 59.7% 39.4%\\n✓ MSE 38.9% 61.7% 41.9%\\n✓ MSE 33.0% 55.4% 35.4%\\n✓ MSE 38.4% 60.7% 41.3%\\n✓ MSE 38.7% 60.7% 41.9%\\n✓ MSE 35.3% 57.2% 38.0%\\n✓ GIoU 39.4% 59.4% 42.5%\\n✓ DIoU 39.1% 58.8% 42.1%\\n✓ CIoU 39.6% 59.2% 42.6%\\n✓ ✓ ✓ ✓ CIoU 41.5% 64.0% 44.8%\\n✓ ✓ ✓ CIoU 36.1% 56.5% 38.4%\\n✓ ✓ ✓ ✓ ✓ MSE 40.3% 64.0% 43.1%\\n✓ ✓ ✓ ✓ ✓ GIoU 42.4% 64.4% 45.9%\\n✓ ✓ ✓ ✓ ✓ CIoU 42.4% 64.4% 45.9%\\nTable 5: Ablation Studies of Bag-of-Specials. (Size 512x512).\\nModel AP AP 50 AP75\\nCSPResNeXt50-PANet-SPP 42.4% 64.4% 45.9%\\nCSPResNeXt50-PANet-SPP-RFB 41.8% 62.7% 45.1%\\nCSPResNeXt50-PANet-SPP-SAM 42.7% 64.6% 46.3%\\nCSPResNeXt50-PANet-SPP-SAM-G 41.6% 62.7% 45.0%\\nCSPResNeXt50-PANet-SPP-ASFF-RFB 41.1% 62.6% 44.4%\\n4.4. Inﬂuence of different backbones and pre-\\ntrained weightings on Detector training\\nFurther on we study the inﬂuence of different backbone\\nmodels on the detector accuracy, as shown in Table 6. We\\nnotice that the model characterized with the best classiﬁca-\\ntion accuracy is not always the best in terms of the detector\\naccuracy.\\nFirst, although classiﬁcation accuracy of CSPResNeXt-\\n50 models trained with different features is higher compared\\nto CSPDarknet53 models, the CSPDarknet53 model shows\\nhigher accuracy in terms of object detection.\\nSecond, using BoF and Mish for the CSPResNeXt50\\nclassiﬁer training increases its classiﬁcation accuracy, but\\nfurther application of these pre-trained weightings for de-\\ntector training reduces the detector accuracy. However, us-\\ning BoF and Mish for the CSPDarknet53 classiﬁer training\\nincreases the accuracy of both the classiﬁer and the detector\\nwhich uses this classiﬁer pre-trained weightings. The net\\nresult is that backbone CSPDarknet53 is more suitable for\\nthe detector than for CSPResNeXt50.\\nWe observe that the CSPDarknet53 model demonstrates\\na greater ability to increase the detector accuracy owing to\\nvarious improvements.Table 6: Using different classiﬁer pre-trained weightings for de-\\ntector training (all other training parameters are similar in all mod-\\nels) .\\nModel (with optimal setting) Size AP AP 50 AP75\\nCSPResNeXt50-PANet-SPP 512x512 42.4 64.4 45.9\\nCSPResNeXt50-PANet-SPP\\n(BoF-backbone)512x512 42.3 64.3 45.7\\nCSPResNeXt50-PANet-SPP\\n(BoF-backbone + Mish)512x512 42.3 64.2 45.8\\nCSPDarknet53-PANet-SPP\\n(BoF-backbone)512x512 42.4 64.5 46.0\\nCSPDarknet53-PANet-SPP\\n(BoF-backbone + Mish)512x512 43.0 64.9 46.5\\n4.5. Inﬂuence of different mini-batch size on Detec-\\ntor training\\nFinally, we analyze the results obtained with models\\ntrained with different mini-batch sizes, and the results are\\nshown in Table 7. From the results shown in Table 7, we\\nfound that after adding BoF and BoS training strategies, the\\nmini-batch size has almost no effect on the detector’s per-\\nformance. This result shows that after the introduction of\\nBoF and BoS, it is no longer necessary to use expensive\\nGPUs for training. In other words, anyone can use only a\\nconventional GPU to train an excellent detector.\\nTable 7: Using different mini-batch size for detector training.\\nModel (without OA) Size AP AP 50 AP75\\nCSPResNeXt50-PANet-SPP\\n(without BoF/BoS, mini-batch 4)608 37.1 59.2 39.9\\nCSPResNeXt50-PANet-SPP\\n(without BoF/BoS, mini-batch 8)608 38.4 60.6 41.6\\nCSPDarknet53-PANet-SPP\\n(with BoF/BoS, mini-batch 4)512 41.6 64.1 45.0\\nCSPDarknet53-PANet-SPP\\n(with BoF/BoS, mini-batch 8)512 41.7 64.2 45.2\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c5ea161d-e40e-4ed0-8ee4-d7e9375895ab', embedding=None, metadata={'page_label': '10', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='1ff66c7d987080bfade10401f88a9ae9911601c47335915712b2b45348d4ce84', text='Figure 8: Comparison of the speed and accuracy of different object detectors. (Some articles stated the FPS of their detectors\\nfor only one of the GPUs: Maxwell/Pascal/V olta)\\n5. Results\\nComparison of the results obtained with other state-\\nof-the-art object detectors are shown in Figure 8. Our\\nYOLOv4 are located on the Pareto optimality curve and are\\nsuperior to the fastest and most accurate detectors in terms\\nof both speed and accuracy.\\nSince different methods use GPUs of different architec-\\ntures for inference time veriﬁcation, we operate YOLOv4\\non commonly adopted GPUs of Maxwell, Pascal, and V olta\\narchitectures, and compare them with other state-of-the-art\\nmethods. Table 8 lists the frame rate comparison results of\\nusing Maxwell GPU, and it can be GTX Titan X (Maxwell)\\nor Tesla M40 GPU. Table 9 lists the frame rate comparison\\nresults of using Pascal GPU, and it can be Titan X (Pascal),\\nTitan Xp, GTX 1080 Ti, or Tesla P100 GPU. As for Table\\n10, it lists the frame rate comparison results of using V olta\\nGPU, and it can be Titan V olta or Tesla V100 GPU.6. Conclusions\\nWe offer a state-of-the-art detector which is faster (FPS)\\nand more accurate (MS COCO AP 50...95and AP 50) than\\nall available alternative detectors. The detector described\\ncan be trained and used on a conventional GPU with 8-16\\nGB-VRAM this makes its broad use possible. The original\\nconcept of one-stage anchor-based detectors has proven its\\nviability. We have veriﬁed a large number of features, and\\nselected for use such of them for improving the accuracy of\\nboth the classiﬁer and the detector. These features can be\\nused as best-practice for future studies and developments.\\n7. Acknowledgements\\nThe authors wish to thank Glenn Jocher for the\\nideas of Mosaic data augmentation, the selection of\\nhyper-parameters by using genetic algorithms and solving\\nthe grid sensitivity problem https://github.com/\\nultralytics/yolov3 .\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='64474ab8-7d28-44b4-8c22-df0fe54f7bdb', embedding=None, metadata={'page_label': '11', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8b99f71acb90dbc1b8a7102e547a5b05ea646861e3e4aa4d6f7fb9b2439aa994', text='Table 8: Comparison of the speed and accuracy of different object detectors on the MS COCO dataset (test-\\ndev 2017). (Real-time detectors with FPS 30 or higher are highlighted here. We compare the results with\\nbatch=1 without using tensorRT.)\\nMethod Backbone Size FPS AP AP 50 AP75 APS APM APL\\nYOLOv4: Optimal Speed and Accuracy of Object Detection\\nYOLOv4 CSPDarknet-53 416 38 (M) 41.2% 62.8% 44.3% 20.4% 44.4% 56.0%\\nYOLOv4 CSPDarknet-53 512 31 (M) 43.0% 64.9% 46.5% 24.3% 46.1% 55.2%\\nYOLOv4 CSPDarknet-53 608 23 (M) 43.5% 65.7% 47.3% 26.7% 46.7% 53.3%\\nLearning Rich Features at High-Speed for Single-Shot Object Detection [84]\\nLRF VGG-16 300 76.9 (M) 32.0% 51.5% 33.8% 12.6% 34.9% 47.0%\\nLRF ResNet-101 300 52.6 (M) 34.3% 54.1% 36.6% 13.2% 38.2% 50.7%\\nLRF VGG-16 512 38.5 (M) 36.2% 56.6% 38.7% 19.0% 39.9% 48.8%\\nLRF ResNet-101 512 31.3 (M) 37.3% 58.5% 39.7% 19.7% 42.8% 50.1%\\nReceptive Field Block Net for Accurate and Fast Object Detection [47]\\nRFBNet VGG-16 300 66.7 (M) 30.3% 49.3% 31.8% 11.8% 31.9% 45.9%\\nRFBNet VGG-16 512 33.3 (M) 33.8% 54.2% 35.9% 16.2% 37.1% 47.4%\\nRFBNet-E VGG-16 512 30.3 (M) 34.4% 55.7% 36.4% 17.6% 37.0% 47.6%\\nYOLOv3: An incremental improvement [63]\\nYOLOv3 Darknet-53 320 45 (M) 28.2% 51.5% 29.7% 11.9% 30.6% 43.4%\\nYOLOv3 Darknet-53 416 35 (M) 31.0% 55.3% 32.3% 15.2% 33.2% 42.8%\\nYOLOv3 Darknet-53 608 20 (M) 33.0% 57.9% 34.4% 18.3% 35.4% 41.9%\\nYOLOv3-SPP Darknet-53 608 20 (M) 36.2% 60.6% 38.2% 20.6% 37.4% 46.1%\\nSSD: Single shot multibox detector [50]\\nSSD VGG-16 300 43 (M) 25.1% 43.1% 25.8% 6.6% 25.9% 41.4%\\nSSD VGG-16 512 22 (M) 28.8% 48.5% 30.3% 10.9% 31.8% 43.5%\\nSingle-shot reﬁnement neural network for object detection [95]\\nReﬁneDet VGG-16 320 38.7 (M) 29.4% 49.2% 31.3% 10.0% 32.0% 44.4%\\nReﬁneDet VGG-16 512 22.3 (M) 33.0% 54.5% 35.5% 16.3% 36.3% 44.3%\\nM2det: A single-shot object detector based on multi-level feature pyramid network [98]\\nM2det VGG-16 320 33.4 (M) 33.5% 52.4% 35.6% 14.4% 37.6% 47.6%\\nM2det ResNet-101 320 21.7 (M) 34.3% 53.5% 36.5% 14.8% 38.8% 47.9%\\nM2det VGG-16 512 18 (M) 37.6% 56.6% 40.5% 18.4% 43.4% 51.2%\\nM2det ResNet-101 512 15.8 (M) 38.8% 59.4% 41.7% 20.5% 43.9% 53.4%\\nM2det VGG-16 800 11.8 (M) 41.0% 59.7% 45.0% 22.1% 46.5% 53.8%\\nParallel Feature Pyramid Network for Object Detection [34]\\nPFPNet-R VGG-16 320 33 (M) 31.8% 52.9% 33.6% 12% 35.5% 46.1%\\nPFPNet-R VGG-16 512 24 (M) 35.2% 57.6% 37.9% 18.7% 38.6% 45.9%\\nFocal Loss for Dense Object Detection [45]\\nRetinaNet ResNet-50 500 13.9 (M) 32.5% 50.9% 34.8% 13.9% 35.8% 46.7%\\nRetinaNet ResNet-101 500 11.1 (M) 34.4% 53.1% 36.8% 14.7% 38.5% 49.1%\\nRetinaNet ResNet-50 800 6.5 (M) 35.7% 55.0% 38.5% 18.9% 38.9% 46.3%\\nRetinaNet ResNet-101 800 5.1 (M) 37.8% 57.5% 40.8% 20.2% 41.1% 49.2%\\nFeature Selective Anchor-Free Module for Single-Shot Object Detection [102]\\nAB+FSAF ResNet-101 800 5.6 (M) 40.9% 61.5% 44.0% 24.0% 44.2% 51.3%\\nAB+FSAF ResNeXt-101 800 2.8 (M) 42.9% 63.8% 46.3% 26.6% 46.2% 52.7%\\nCornerNet: Detecting objects as paired keypoints [37]\\nCornerNet Hourglass 512 4.4 (M) 40.5% 57.8% 45.3% 20.8% 44.8% 56.7%\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='94f916fc-934e-419e-932f-450ff17d79a4', embedding=None, metadata={'page_label': '12', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7e955b878b230e2917bb747e8cf0c3c8312fe66bf1bbe77434c643b5da19d869', text='Table 9: Comparison of the speed and accuracy of different object detectors on the MS COCO dataset (test-dev 2017).\\n(Real-time detectors with FPS 30 or higher are highlighted here. We compare the results with batch=1 without using\\ntensorRT.)\\nMethod Backbone Size FPS AP AP 50 AP75 APS APM APL\\nYOLOv4: Optimal Speed and Accuracy of Object Detection\\nYOLOv4 CSPDarknet-53 416 54 (P) 41.2% 62.8% 44.3% 20.4% 44.4% 56.0%\\nYOLOv4 CSPDarknet-53 512 43 (P) 43.0% 64.9% 46.5% 24.3% 46.1% 55.2%\\nYOLOv4 CSPDarknet-53 608 33 (P) 43.5% 65.7% 47.3% 26.7% 46.7% 53.3%\\nCenterMask: Real-Time Anchor-Free Instance Segmentation [40]\\nCenterMask-Lite MobileNetV2-FPN 600× 50.0 (P) 30.2% - - 14.2% 31.9% 40.9%\\nCenterMask-Lite V oVNet-19-FPN 600× 43.5 (P) 35.9% - - 19.6% 38.0% 45.9%\\nCenterMask-Lite V oVNet-39-FPN 600× 35.7 (P) 40.7% - - 22.4% 43.2% 53.5%\\nEnriched Feature Guided Reﬁnement Network for Object Detection [57]\\nEFGRNet VGG-16 320 47.6 (P) 33.2% 53.4% 35.4% 13.4% 37.1% 47.9%\\nEFGRNet VG-G16 512 25.7 (P) 37.5% 58.8% 40.4% 19.7% 41.6% 49.4%\\nEFGRNet ResNet-101 512 21.7 (P) 39.0% 58.8% 42.3% 17.8% 43.6% 54.5%\\nHierarchical Shot Detector [3]\\nHSD VGG-16 320 40 (P) 33.5% 53.2% 36.1% 15.0% 35.0% 47.8%\\nHSD VGG-16 512 23.3 (P) 38.8% 58.2% 42.5% 21.8% 41.9% 50.2%\\nHSD ResNet-101 512 20.8 (P) 40.2% 59.4% 44.0% 20.0% 44.4% 54.9%\\nHSD ResNeXt-101 512 15.2 (P) 41.9% 61.1% 46.2% 21.8% 46.6% 57.0%\\nHSD ResNet-101 768 10.9 (P) 42.3% 61.2% 46.9% 22.8% 47.3% 55.9%\\nDynamic anchor feature selection for single-shot object detection [41]\\nDAFS VGG16 512 35 (P) 33.8% 52.9% 36.9% 14.6% 37.0% 47.7%\\nSoft Anchor-Point Object Detection [101]\\nSAPD ResNet-50 - 14.9 (P) 41.7% 61.9% 44.6% 24.1% 44.6% 51.6%\\nSAPD ResNet-50-DCN - 12.4 (P) 44.3% 64.4% 47.7% 25.5% 47.3% 57.0%\\nSAPD ResNet-101-DCN - 9.1 (P) 46.0% 65.9% 49.6% 26.3% 49.2% 59.6%\\nRegion proposal by guided anchoring [82]\\nRetinaNet ResNet-50 - 10.8 (P) 37.1% 56.9% 40.0% 20.1% 40.1% 48.0%\\nFaster R-CNN ResNet-50 - 9.4 (P) 39.8% 59.2% 43.5% 21.8% 42.6% 50.7%\\nRepPoints: Point set representation for object detection [87]\\nRPDet ResNet-101 - 10 (P) 41.0% 62.9% 44.3% 23.6% 44.1% 51.7%\\nRPDet ResNet-101-DCN - 8 (P) 45.0% 66.1% 49.0% 26.6% 48.6% 57.5%\\nLibra R-CNN: Towards balanced learning for object detection [58]\\nLibra R-CNN ResNet-101 - 9.5 (P) 41.1% 62.1% 44.7% 23.4% 43.7% 52.5%\\nFreeAnchor: Learning to match anchors for visual object detection [96]\\nFreeAnchor ResNet-101 - 9.1 (P) 43.1% 62.2% 46.4% 24.5% 46.1% 54.8%\\nRetinaMask: Learning to Predict Masks Improves State-of-The-Art Single-Shot Detection for Free [14]\\nRetinaMask ResNet-50-FPN 800 × 8.1 (P) 39.4% 58.6% 42.3% 21.9% 42.0% 51.0%\\nRetinaMask ResNet-101-FPN 800 × 6.9 (P) 41.4% 60.8% 44.6% 23.0% 44.5% 53.5%\\nRetinaMask ResNet-101-FPN-GN 800 × 6.5 (P) 41.7% 61.7% 45.0% 23.5% 44.7% 52.8%\\nRetinaMask ResNeXt-101-FPN-GN 800 × 4.3 (P) 42.6% 62.5% 46.0% 24.8% 45.6% 53.8%\\nCascade R-CNN: Delving into high quality object detection [2]\\nCascade R-CNN ResNet-101 - 8 (P) 42.8% 62.1% 46.3% 23.7% 45.5% 55.2%\\nCenternet: Object detection with keypoint triplets [13]\\nCenternet Hourglass-52 - 4.4 (P) 41.6% 59.4% 44.2% 22.5% 43.1% 54.1%\\nCenternet Hourglass-104 - 3.3 (P) 44.9% 62.4% 48.1% 25.6% 47.4% 57.4%\\nScale-Aware Trident Networks for Object Detection [42]\\nTridentNet ResNet-101 - 2.7 (P) 42.7% 63.6% 46.5% 23.9% 46.6% 56.6%\\nTridentNet ResNet-101-DCN - 1.3 (P) 46.8% 67.6% 51.5% 28.0% 51.2% 60.5%\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='61163683-a791-4c2a-a7b1-53fad4be0666', embedding=None, metadata={'page_label': '13', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='81cd5c5f42b05e9499e6759b6211e2c9b35ca0ef318ac3b8d16fca35fadc06fd', text='Table 10: Comparison of the speed and accuracy of different object detectors on the MS COCO dataset (test-dev 2017).\\n(Real-time detectors with FPS 30 or higher are highlighted here. We compare the results with batch=1 without using\\ntensorRT.)\\nMethod Backbone Size FPS AP AP 50 AP75 APS APM APL\\nYOLOv4: Optimal Speed and Accuracy of Object Detection\\nYOLOv4 CSPDarknet-53 416 96 (V) 41.2% 62.8% 44.3% 20.4% 44.4% 56.0%\\nYOLOv4 CSPDarknet-53 512 83 (V) 43.0% 64.9% 46.5% 24.3% 46.1% 55.2%\\nYOLOv4 CSPDarknet-53 608 62 (V) 43.5% 65.7% 47.3% 26.7% 46.7% 53.3%\\nEfﬁcientDet: Scalable and Efﬁcient Object Detection [77]\\nEfﬁcientDet-D0 Efﬁcient-B0 512 62.5 (V) 33.8% 52.2% 35.8% 12.0% 38.3% 51.2%\\nEfﬁcientDet-D1 Efﬁcient-B1 640 50.0 (V) 39.6% 58.6% 42.3% 17.9% 44.3% 56.0%\\nEfﬁcientDet-D2 Efﬁcient-B2 768 41.7 (V) 43.0% 62.3% 46.2% 22.5% 47.0% 58.4%\\nEfﬁcientDet-D3 Efﬁcient-B3 896 23.8 (V) 45.8% 65.0% 49.3% 26.6% 49.4% 59.8%\\nLearning Spatial Fusion for Single-Shot Object Detection [48]\\nYOLOv3 + ASFF* Darknet-53 320 60 (V) 38.1% 57.4% 42.1% 16.1% 41.6% 53.6%\\nYOLOv3 + ASFF* Darknet-53 416 54 (V) 40.6% 60.6% 45.1% 20.3% 44.2% 54.1%\\nYOLOv3 + ASFF* Darknet-53 608× 45.5 (V) 42.4% 63.0% 47.4% 25.5% 45.7% 52.3%\\nYOLOv3 + ASFF* Darknet-53 800 × 29.4 (V) 43.9% 64.1% 49.2% 27.0% 46.6% 53.4%\\nHarDNet: A Low Memory Trafﬁc Network [4]\\nRFBNet HarDNet68 512 41.5 (V) 33.9% 54.3% 36.2% 14.7% 36.6% 50.5%\\nRFBNet HarDNet85 512 37.1 (V) 36.8% 57.1% 39.5% 16.9% 40.5% 52.9%\\nFocal Loss for Dense Object Detection [45]\\nRetinaNet ResNet-50 640 37 (V) 37.0% - - - - -\\nRetinaNet ResNet-101 640 29.4 (V) 37.9% - - - - -\\nRetinaNet ResNet-50 1024 19.6 (V) 40.1% - - - - -\\nRetinaNet ResNet-101 1024 15.4 (V) 41.1% - - - - -\\nSM-NAS: Structural-to-Modular Neural Architecture Search for Object Detection [88]\\nSM-NAS: E2 - 800 ×600 25.3 (V) 40.0% 58.2% 43.4% 21.1% 42.4% 51.7%\\nSM-NAS: E3 - 800 ×600 19.7 (V) 42.8% 61.2% 46.5% 23.5% 45.5% 55.6%\\nSM-NAS: E5 - 1333 ×800 9.3 (V) 45.9% 64.6% 49.6% 27.1% 49.0% 58.0%\\nNAS-FPN: Learning scalable feature pyramid architecture for object detection [17]\\nNAS-FPN ResNet-50 640 24.4 (V) 39.9% - - - - -\\nNAS-FPN ResNet-50 1024 12.7 (V) 44.2% - - - - -\\nBridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection [94]\\nATSS ResNet-101 800 × 17.5 (V) 43.6% 62.1% 47.4% 26.1% 47.0% 53.6%\\nATSS ResNet-101-DCN 800 × 13.7 (V) 46.3% 64.7% 50.4% 27.7% 49.8% 58.4%\\nRDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation [83]\\nRDSNet ResNet-101 600 16.8 (V) 36.0% 55.2% 38.7% 17.4% 39.6% 49.7%\\nRDSNet ResNet-101 800 10.9 (V) 38.1% 58.5% 40.8% 21.2% 41.5% 48.2%\\nCenterMask: Real-Time Anchor-Free Instance Segmentation [40]\\nCenterMask ResNet-101-FPN 800 × 15.2 (V) 44.0% - - 25.8% 46.8% 54.9%\\nCenterMask V oVNet-99-FPN 800 × 12.9 (V) 46.5% - - 28.7% 48.9% 57.2%\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e7975edb-0015-4412-9ade-e17d92715b0d', embedding=None, metadata={'page_label': '14', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='29323e95ff97282213a5d05db472c569bc182d82e7d2dc25cc22f2e9a0e04225', text='References\\n[1] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and\\nLarry S Davis. Soft-NMS–improving object detection with\\none line of code. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV) , pages 5561–5569,\\n2017. 4\\n[2] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN:\\nDelving into high quality object detection. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 6154–6162, 2018. 12\\n[3] Jiale Cao, Yanwei Pang, Jungong Han, and Xuelong Li. Hi-\\nerarchical shot detector. In Proceedings of the IEEE In-\\nternational Conference on Computer Vision (ICCV) , pages\\n9705–9714, 2019. 12\\n[4] Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang\\nHuang, and Youn-Long Lin. HarDNet: A low memory traf-\\nﬁc network. Proceedings of the IEEE International Confer-\\nence on Computer Vision (ICCV) , 2019. 13\\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\\nKevin Murphy, and Alan L Yuille. DeepLab: Semantic im-\\nage segmentation with deep convolutional nets, atrous con-\\nvolution, and fully connected CRFs. IEEE Transactions\\non Pattern Analysis and Machine Intelligence (TPAMI) ,\\n40(4):834–848, 2017. 2, 4\\n[6] Pengguang Chen. GridMask data augmentation. arXiv\\npreprint arXiv:2001.04086 , 2020. 3\\n[7] Yukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng,\\nXinyu Xiao, and Jian Sun. DetNAS: Backbone search for\\nobject detection. In Advances in Neural Information Pro-\\ncessing Systems (NeurIPS) , pages 6638–6648, 2019. 2\\n[8] Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae\\nLee. Gaussian YOLOv3: An accurate and fast object de-\\ntector using localization uncertainty for autonomous driv-\\ning. In Proceedings of the IEEE International Conference\\non Computer Vision (ICCV) , pages 502–511, 2019. 7\\n[9] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN:\\nObject detection via region-based fully convolutional net-\\nworks. In Advances in Neural Information Processing Sys-\\ntems (NIPS) , pages 379–387, 2016. 2\\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. ImageNet: A large-scale hierarchical im-\\nage database. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n248–255, 2009. 5\\n[11] Terrance DeVries and Graham W Taylor. Improved reg-\\nularization of convolutional neural networks with CutOut.\\narXiv preprint arXiv:1708.04552 , 2017. 3\\n[12] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi,\\nMingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song.\\nSpineNet: Learning scale-permuted backbone for recog-\\nnition and localization. arXiv preprint arXiv:1912.05027 ,\\n2019. 2\\n[13] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qing-\\nming Huang, and Qi Tian. CenterNet: Keypoint triplets for\\nobject detection. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV) , pages 6569–6578,\\n2019. 2, 12[14] Cheng-Yang Fu, Mykhailo Shvets, and Alexander C Berg.\\nRetinaMask: Learning to predict masks improves state-\\nof-the-art single-shot detection for free. arXiv preprint\\narXiv:1901.03353 , 2019. 12\\n[15] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,\\nMatthias Bethge, Felix A Wichmann, and Wieland Brendel.\\nImageNet-trained cnns are biased towards texture; increas-\\ning shape bias improves accuracy and robustness. In Inter-\\nnational Conference on Learning Representations (ICLR) ,\\n2019. 3\\n[16] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. DropBlock:\\nA regularization method for convolutional networks. In Ad-\\nvances in Neural Information Processing Systems (NIPS) ,\\npages 10727–10737, 2018. 3\\n[17] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. NAS-FPN:\\nLearning scalable feature pyramid architecture for object\\ndetection. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 7036–\\n7045, 2019. 2, 13\\n[18] Ross Girshick. Fast R-CNN. In Proceedings of the IEEE In-\\nternational Conference on Computer Vision (ICCV) , pages\\n1440–1448, 2015. 2\\n[19] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\\nMalik. Rich feature hierarchies for accurate object de-\\ntection and semantic segmentation. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition (CVPR) , pages 580–587, 2014. 2, 4\\n[20] Jianyuan Guo, Kai Han, Yunhe Wang, Chao Zhang, Zhao-\\nhui Yang, Han Wu, Xinghao Chen, and Chang Xu. Hit-\\nDetector: Hierarchical trinity architecture search for object\\ndetection. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , 2020. 2\\n[21] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing\\nXu, and Chang Xu. GhostNet: More features from cheap\\noperations. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , 2020.\\n5\\n[22] Bharath Hariharan, Pablo Arbel ´aez, Ross Girshick, and\\nJitendra Malik. Hypercolumns for object segmentation\\nand ﬁne-grained localization. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 447–456, 2015. 4\\n[23] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\\nshick. Mask R-CNN. In Proceedings of the IEEE In-\\nternational Conference on Computer Vision (ICCV) , pages\\n2961–2969, 2017. 2\\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDelving deep into rectiﬁers: Surpassing human-level per-\\nformance on ImageNet classiﬁcation. In Proceedings of\\nthe IEEE International Conference on Computer Vision\\n(ICCV) , pages 1026–1034, 2015. 4\\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nSpatial pyramid pooling in deep convolutional networks for\\nvisual recognition. IEEE Transactions on Pattern Analy-\\nsis and Machine Intelligence (TPAMI) , 37(9):1904–1916,\\n2015. 2, 4, 7\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='31087c10-7854-4fcb-996d-a5823a604633', embedding=None, metadata={'page_label': '15', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='719e9c74a4af520f224687fa87695a24e34700fe4fb952eeb7e012d8587e0bf4', text='ings of the IEEE Conference on Computer Vision and Pat-\\ntern Recognition (CVPR) , pages 770–778, 2016. 2\\n[27] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for Mo-\\nbileNetV3. In Proceedings of the IEEE International Con-\\nference on Computer Vision (ICCV) , 2019. 2, 4\\n[28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. MobileNets: Efﬁcient con-\\nvolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861 , 2017. 2, 4\\n[29] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\\nnetworks. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 7132–\\n7141, 2018. 4\\n[30] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 4700–\\n4708, 2017. 2\\n[31] Forrest N Iandola, Song Han, Matthew W Moskewicz,\\nKhalid Ashraf, William J Dally, and Kurt Keutzer.\\nSqueezeNet: AlexNet-level accuracy with 50x fewer pa-\\nrameters and¡ 0.5 MB model size. arXiv preprint\\narXiv:1602.07360 , 2016. 2\\n[32] Sergey Ioffe and Christian Szegedy. Batch normalization:\\nAccelerating deep network training by reducing internal co-\\nvariate shift. arXiv preprint arXiv:1502.03167 , 2015. 6\\n[33] Md Amirul Islam, Shujon Naha, Mrigank Rochan, Neil\\nBruce, and Yang Wang. Label reﬁnement network for\\ncoarse-to-ﬁne semantic segmentation. arXiv preprint\\narXiv:1703.00551 , 2017. 3\\n[34] Seung-Wook Kim, Hyong-Keun Kook, Jee-Young Sun,\\nMun-Cheon Kang, and Sung-Jea Ko. Parallel feature pyra-\\nmid network for object detection. In Proceedings of the\\nEuropean Conference on Computer Vision (ECCV) , pages\\n234–250, 2018. 11\\n[35] G ¨unter Klambauer, Thomas Unterthiner, Andreas Mayr,\\nand Sepp Hochreiter. Self-normalizing neural networks.\\nInAdvances in Neural Information Processing Systems\\n(NIPS) , pages 971–980, 2017. 4\\n[36] Gustav Larsson, Michael Maire, and Gregory\\nShakhnarovich. FractalNet: Ultra-deep neural net-\\nworks without residuals. arXiv preprint arXiv:1605.07648 ,\\n2016. 6\\n[37] Hei Law and Jia Deng. CornerNet: Detecting objects as\\npaired keypoints. In Proceedings of the European Confer-\\nence on Computer Vision (ECCV) , pages 734–750, 2018. 2,\\n11\\n[38] Hei Law, Yun Teng, Olga Russakovsky, and Jia Deng.\\nCornerNet-Lite: Efﬁcient keypoint based object detection.\\narXiv preprint arXiv:1904.08900 , 2019. 2\\n[39] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Be-\\nyond bags of features: Spatial pyramid matching for recog-\\nnizing natural scene categories. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , volume 2, pages 2169–2178. IEEE, 2006. 4[40] Youngwan Lee and Jongyoul Park. CenterMask: Real-time\\nanchor-free instance segmentation. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition (CVPR) , 2020. 12, 13\\n[41] Shuai Li, Lingxiao Yang, Jianqiang Huang, Xian-Sheng\\nHua, and Lei Zhang. Dynamic anchor feature selection for\\nsingle-shot object detection. In Proceedings of the IEEE In-\\nternational Conference on Computer Vision (ICCV) , pages\\n6609–6618, 2019. 12\\n[42] Yanghao Li, Yuntao Chen, Naiyan Wang, and Zhaoxiang\\nZhang. Scale-aware trident networks for object detection.\\nInProceedings of the IEEE International Conference on\\nComputer Vision (ICCV) , pages 6054–6063, 2019. 12\\n[43] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yang-\\ndong Deng, and Jian Sun. DetNet: Design backbone for\\nobject detection. In Proceedings of the European Confer-\\nence on Computer Vision (ECCV) , pages 334–350, 2018.\\n2\\n[44] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie. Feature pyramid\\nnetworks for object detection. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 2117–2125, 2017. 2\\n[45] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\\nand Piotr Doll ´ar. Focal loss for dense object detection. In\\nProceedings of the IEEE International Conference on Com-\\nputer Vision (ICCV) , pages 2980–2988, 2017. 2, 3, 11, 13\\n[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\\nC Lawrence Zitnick. Microsoft COCO: Common objects\\nin context. In Proceedings of the European Conference on\\nComputer Vision (ECCV) , pages 740–755, 2014. 5\\n[47] Songtao Liu, Di Huang, et al. Receptive ﬁeld block net for\\naccurate and fast object detection. In Proceedings of the\\nEuropean Conference on Computer Vision (ECCV) , pages\\n385–400, 2018. 2, 4, 11\\n[48] Songtao Liu, Di Huang, and Yunhong Wang. Learning spa-\\ntial fusion for single-shot object detection. arXiv preprint\\narXiv:1911.09516 , 2019. 2, 4, 13\\n[49] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\\nPath aggregation network for instance segmentation. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 8759–8768, 2018.\\n1, 2, 7\\n[50] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\\nBerg. SSD: Single shot multibox detector. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 21–37, 2016. 2, 11\\n[51] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\\nconvolutional networks for semantic segmentation. In Pro-\\nceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 3431–3440, 2015. 4\\n[52] Ilya Loshchilov and Frank Hutter. SGDR: Stochas-\\ntic gradient descent with warm restarts. arXiv preprint\\narXiv:1608.03983 , 2016. 7\\n[53] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian\\nSun. ShufﬂeNetV2: Practical guidelines for efﬁcient cnn\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cf82ad9f-6d48-4c96-b69a-2167cf00b39e', embedding=None, metadata={'page_label': '16', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='f611322c992d73b72c144cdea524bb1d486a3789f8fde174c7cbc4b6333e1960', text='architecture design. In Proceedings of the European Con-\\nference on Computer Vision (ECCV) , pages 116–131, 2018.\\n2\\n[54] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rec-\\ntiﬁer nonlinearities improve neural network acoustic mod-\\nels. In Proceedings of International Conference on Ma-\\nchine Learning (ICML) , volume 30, page 3, 2013. 4\\n[55] Diganta Misra. Mish: A self regularized non-\\nmonotonic neural activation function. arXiv preprint\\narXiv:1908.08681 , 2019. 4\\n[56] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units\\nimprove restricted boltzmann machines. In Proceedings\\nof International Conference on Machine Learning (ICML) ,\\npages 807–814, 2010. 4\\n[57] Jing Nie, Rao Muhammad Anwer, Hisham Cholakkal, Fa-\\nhad Shahbaz Khan, Yanwei Pang, and Ling Shao. Enriched\\nfeature guided reﬁnement network for object detection. In\\nProceedings of the IEEE International Conference on Com-\\nputer Vision (ICCV) , pages 9537–9546, 2019. 12\\n[58] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng,\\nWanli Ouyang, and Dahua Lin. Libra R-CNN: Towards bal-\\nanced learning for object detection. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition (CVPR) , pages 821–830, 2019. 2, 12\\n[59] Prajit Ramachandran, Barret Zoph, and Quoc V Le.\\nSearching for activation functions. arXiv preprint\\narXiv:1710.05941 , 2017. 4\\n[60] Abdullah Rashwan, Agastya Kalra, and Pascal Poupart.\\nMatrix Nets: A new deep architecture for object detection.\\nInProceedings of the IEEE International Conference on\\nComputer Vision Workshop (ICCV Workshop) , pages 0–0,\\n2019. 2\\n[61] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Uniﬁed, real-time object de-\\ntection. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 779–\\n788, 2016. 2\\n[62] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 7263–\\n7271, 2017. 2\\n[63] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767 , 2018. 2,\\n4, 7, 11\\n[64] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\\nFaster R-CNN: Towards real-time object detection with re-\\ngion proposal networks. In Advances in Neural Information\\nProcessing Systems (NIPS) , pages 91–99, 2015. 2\\n[65] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding\\nbox regression. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n658–666, 2019. 3\\n[66] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey\\nZhmoginov, and Liang-Chieh Chen. MobileNetV2: In-\\nverted residuals and linear bottlenecks. In Proceedingsof the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 4510–4520, 2018. 2\\n[67] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.\\nTraining region-based object detectors with online hard ex-\\nample mining. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n761–769, 2016. 3\\n[68] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556 , 2014. 2\\n[69] Krishna Kumar Singh, Hao Yu, Aron Sarmasi, Gautam\\nPradeep, and Yong Jae Lee. Hide-and-Seek: A data aug-\\nmentation technique for weakly-supervised localization and\\nbeyond. arXiv preprint arXiv:1811.02545 , 2018. 3\\n[70] Saurabh Singh and Shankar Krishnan. Filter response\\nnormalization layer: Eliminating batch dependence in\\nthe training of deep neural networks. arXiv preprint\\narXiv:1911.09737 , 2019. 6\\n[71] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\\nSutskever, and Ruslan Salakhutdinov. DropOut: A simple\\nway to prevent neural networks from overﬁtting. The jour-\\nnal of machine learning research , 15(1):1929–1958, 2014.\\n3\\n[72] K-K Sung and Tomaso Poggio. Example-based learning\\nfor view-based human face detection. IEEE Transactions\\non Pattern Analysis and Machine Intelligence (TPAMI) ,\\n20(1):39–51, 1998. 3\\n[73] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\\nchitecture for computer vision. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 2818–2826, 2016. 3\\n[74] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\\nMark Sandler, Andrew Howard, and Quoc V Le. MNAS-\\nnet: Platform-aware neural architecture search for mobile.\\nInProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 2820–2828, 2019.\\n2\\n[75] Mingxing Tan and Quoc V Le. EfﬁcientNet: Rethinking\\nmodel scaling for convolutional neural networks. In Pro-\\nceedings of International Conference on Machine Learning\\n(ICML) , 2019. 2\\n[76] Mingxing Tan and Quoc V Le. MixNet: Mixed depthwise\\nconvolutional kernels. In Proceedings of the British Ma-\\nchine Vision Conference (BMVC) , 2019. 5\\n[77] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcient-\\nDet: Scalable and efﬁcient object detection. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) , 2020. 2, 4, 13\\n[78] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. In Proceed-\\nings of the IEEE International Conference on Computer Vi-\\nsion (ICCV) , pages 9627–9636, 2019. 2\\n[79] Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann Le-\\nCun, and Christoph Bregler. Efﬁcient object localization\\nusing convolutional networks. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 648–656, 2015. 6\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='90f58996-826d-44c5-bea1-d9174ecd8fd3', embedding=None, metadata={'page_label': '17', 'file_name': '2004.10934.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e81a778b11024d6cd963042f616f826329af53a60e50b3a450524b73993b2b95', text='[80] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and\\nRob Fergus. Regularization of neural networks using Drop-\\nConnect. In Proceedings of International Conference on\\nMachine Learning (ICML) , pages 1058–1066, 2013. 3\\n[81] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet:\\nA new backbone that can enhance learning capability of\\ncnn. Proceedings of the IEEE Conference on Computer Vi-\\nsion and Pattern Recognition Workshop (CVPR Workshop) ,\\n2020. 2, 7\\n[82] Jiaqi Wang, Kai Chen, Shuo Yang, Chen Change Loy, and\\nDahua Lin. Region proposal by guided anchoring. In Pro-\\nceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 2965–2974, 2019. 12\\n[83] Shaoru Wang, Yongchao Gong, Junliang Xing, Lichao\\nHuang, Chang Huang, and Weiming Hu. RDSNet: A\\nnew deep architecture for reciprocal object detection and\\ninstance segmentation. arXiv preprint arXiv:1912.05070 ,\\n2019. 13\\n[84] Tiancai Wang, Rao Muhammad Anwer, Hisham Cholakkal,\\nFahad Shahbaz Khan, Yanwei Pang, and Ling Shao. Learn-\\ning rich features at high-speed for single-shot object detec-\\ntion. In Proceedings of the IEEE International Conference\\non Computer Vision (ICCV) , pages 1971–1980, 2019. 11\\n[85] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In\\nSo Kweon. CBAM: Convolutional block attention module.\\nInProceedings of the European Conference on Computer\\nVision (ECCV) , pages 3–19, 2018. 1, 2, 4\\n[86] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1492–1500, 2017. 2\\n[87] Ze Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen\\nLin. RepPoints: Point set representation for object detec-\\ntion. In Proceedings of the IEEE International Conference\\non Computer Vision (ICCV) , pages 9657–9666, 2019. 2, 12\\n[88] Lewei Yao, Hang Xu, Wei Zhang, Xiaodan Liang, and\\nZhenguo Li. SM-NAS: Structural-to-modular neural archi-\\ntecture search for object detection. In Proceedings of the\\nAAAI Conference on Artiﬁcial Intelligence (AAAI) , 2020.\\n13\\n[89] Zhuliang Yao, Yue Cao, Shuxin Zheng, Gao Huang, and\\nStephen Lin. Cross-iteration batch normalization. arXiv\\npreprint arXiv:2002.05712 , 2020. 1, 6\\n[90] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao,\\nand Thomas Huang. UnitBox: An advanced object detec-\\ntion network. In Proceedings of the 24th ACM international\\nconference on Multimedia , pages 516–520, 2016. 3\\n[91] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\\nChun, Junsuk Choe, and Youngjoon Yoo. CutMix: Regu-\\nlarization strategy to train strong classiﬁers with localizable\\nfeatures. In Proceedings of the IEEE International Confer-\\nence on Computer Vision (ICCV) , pages 6023–6032, 2019.\\n3\\n[92] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\\nDavid Lopez-Paz. MixUp: Beyond empirical risk mini-\\nmization. arXiv preprint arXiv:1710.09412 , 2017. 3[93] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang,\\nXiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Con-\\ntext encoding for semantic segmentation. In Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 7151–7160, 2018. 6\\n[94] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\\nStan Z Li. Bridging the gap between anchor-based and\\nanchor-free detection via adaptive training sample selec-\\ntion. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR) , 2020. 13\\n[95] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and\\nStan Z Li. Single-shot reﬁnement neural network for ob-\\nject detection. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4203–4212, 2018. 11\\n[96] Xiaosong Zhang, Fang Wan, Chang Liu, Rongrong Ji, and\\nQixiang Ye. FreeAnchor: Learning to match anchors for\\nvisual object detection. In Advances in Neural Information\\nProcessing Systems (NeurIPS) , 2019. 12\\n[97] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\\nShufﬂeNet: An extremely efﬁcient convolutional neural\\nnetwork for mobile devices. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 6848–6856, 2018. 2\\n[98] Qijie Zhao, Tao Sheng, Yongtao Wang, Zhi Tang, Ying\\nChen, Ling Cai, and Haibin Ling. M2det: A single-shot\\nobject detector based on multi-level feature pyramid net-\\nwork. In Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence (AAAI) , volume 33, pages 9259–9266, 2019. 2,\\n4, 11\\n[99] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU Loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence (AAAI) ,\\n2020. 3, 4\\n[100] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li,\\nand Yi Yang. Random erasing data augmentation. arXiv\\npreprint arXiv:1708.04896 , 2017. 3\\n[101] Chenchen Zhu, Fangyi Chen, Zhiqiang Shen, and Mar-\\nios Savvides. Soft anchor-point object detection. arXiv\\npreprint arXiv:1911.12448 , 2019. 12\\n[102] Chenchen Zhu, Yihui He, and Marios Savvides. Feature se-\\nlective anchor-free module for single-shot object detection.\\nInProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , pages 840–849, 2019. 11\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyCyU50XJ2tKAbfaunR7ER2YmzfDvXBH7uc'"
      ],
      "metadata": {
        "id": "ZfoaGTJnaZAk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = PaLM()"
      ],
      "metadata": {
        "id": "T5hb4zAtagZu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Service Context\n",
        "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=800, chunk_overlap=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "78b9fd250fa94c12badd1eaf16b3b0d1",
            "7ae138fc1a6744b79df1518f7fd349c5",
            "72e577b44dd242daaa9adfd717645ed4",
            "65c973d85f934410bbfbe4bbd34259a7",
            "71469dc4c7b24e39b9a9ba222090d165",
            "ebd204629ae44ddca57d433a7ca20476",
            "fecd58251f45415ab86708b2f2b80f08",
            "ac5c1b509fd4407b8d86e79fb675156f",
            "076a0eb9c9df40a09f7c4707f3e628a5",
            "8e8984485dc54633947ab084cfce142f",
            "933eae071a05441a9751db8812675c87",
            "86ef5d756ed741c4be430305fb7f1078",
            "85d7a8e5ed1e4ba5a6440bfa147c3611",
            "aabcf9288b1a4cc09a4b916d2b721b66",
            "203be9f1d261470d918709f8b6e88bd1",
            "3ee9cba4e0204269a9b7775c93f59772",
            "0659fa2c7f754637987eb63f92fa475f",
            "9feafcad7674449fa3e3120e1598b848",
            "18b19b21d30d43b796e86baed3a774e4",
            "a8d5a910a76048938183dbc6ad654ab6",
            "68adfe7fd88d4efc9569d1148cd7898c",
            "7daff023fa8d42ad87c9261aee441895",
            "9ae9bec960d74eeb80bad652000c5580",
            "ee19145339454cec87167b90c31badc5",
            "62edc9af85f9424791b02ff19b1f3a48",
            "d6f05e62d3754514b20215095eed4e49",
            "f2e18eea3d944efe9163714ac6cf9aed",
            "122229f20e1e46f6a302e8177104a82a",
            "319e410535144f498bfc4af5ff79c90f",
            "8cfafb4c1ef244d7b05ffde5f4d7778c",
            "2e38cd49ad404e92a7ef4a5c8280893e",
            "49b1f04c6e924e8d8692b00aba5e76b4",
            "c0a2588c629e42e5abb08e1c5f7d9489",
            "9c2665f7a29b43439be156d12c1694d8",
            "b08b8368da0344b89e6091e77e43990d",
            "8f953d7b5dda4eab85786e1ae4b4c3de",
            "3a126fdb6ade425683dc27f763f359bb",
            "380db9ac33d64921bd8ad2f7d2310c33",
            "0f3dfd76e71f406aa6d2f447792ab055",
            "fcc8ffa5b13b4c88b46311c45fe8ff75",
            "ad49b3a055c24d2fb4ea0abdc20a7bb1",
            "e803d0b0560d4efc8d30e0643392dea5",
            "a0aea0d6d4254195a44ccaaa59a3efb6",
            "27548cc9b9224bf8b9306e7b78fd1ddf",
            "6ed5a562bb454e9cb30350ef8ccb6483",
            "06d45b512dab421d9052af5e7b170d22",
            "223e66fd26fc4883a980cb80b886a7e5",
            "5415569985394a74b1f0a034142c1fa0",
            "f459acedc6ee4cf1963065aafb7bd2d4",
            "676693eb60e442bca60f967d5df3bca7",
            "36fc378524ea4178a4b754e29ba9efdb",
            "dd7e41f0078e480c9120aaeffd32d44c",
            "9f8f3ffeb4aa45cf93014a6ae1a518c5",
            "d93ded780dbf451e9fc2fb3c5165e99e",
            "4127b07d1223492598594fc971b1ae76",
            "8105c5e63ccb44ae906f5913f39569db",
            "5cbb9e103e854f1887d9d0bca90ff2d3",
            "1ba62b57453d4bca9c7e1907e23d1464",
            "839e7082f2bf4458b5ac3b7cd3e88a74",
            "28bbb306537d4209a456e1cb8e0743e8",
            "62542ee915894b90820128add69062ee",
            "02964643d2c34a629cb56a0f580e5f7c",
            "c6ee799b2a6a4b5d80e143070803dfac",
            "8eb61379d8914640a2ca93df71863f8a",
            "1da75927c62b45ccb70c68f56f7f8c71",
            "ca836b0801e045a386bfb9214fe77def"
          ]
        },
        "id": "g2oSTt9Mai5n",
        "outputId": "ea1a5b6d-53af-45bb-a43b-c7af9a256798"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******\n",
            "Could not load OpenAIEmbedding. Using HuggingFaceBgeEmbeddings with model_name=BAAI/bge-small-en. If you intended to use OpenAI, please check your OPENAI_API_KEY.\n",
            "Original error:\n",
            "No API key found for OpenAI.\n",
            "Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\n",
            "API keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\n",
            "******\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78b9fd250fa94c12badd1eaf16b3b0d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86ef5d756ed741c4be430305fb7f1078"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ae9bec960d74eeb80bad652000c5580"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c2665f7a29b43439be156d12c1694d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ed5a562bb454e9cb30350ef8ccb6483"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8105c5e63ccb44ae906f5913f39569db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "RofWgkb2aow0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist()\n",
        ""
      ],
      "metadata": {
        "id": "I7h7qvUDaylO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"YOLOv4\")"
      ],
      "metadata": {
        "id": "VZ_rY32RazwO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(f\"{response}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "LN1PJ5QTa5Cd",
        "outputId": "4efb6c57-06d1-415f-aba9-9e43ffe3bf7f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "YOLOv4 is an efficient and powerful object detection model. It makes everyone can use a 1080 Ti or 2080 Ti GPU to train a super fast and accurate object detector."
          },
          "metadata": {}
        }
      ]
    }
  ]
}